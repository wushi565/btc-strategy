"""
æ¨¡å‹è®­ç»ƒæ¨¡å—

å®ç°éœ€æ±‚1ä¸­çš„æ¨¡å‹è®­ç»ƒåŠŸèƒ½ï¼š
- å¤šç§æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
- è¶…å‚æ•°ä¼˜åŒ–
- äº¤å‰éªŒè¯è¯„ä¼°
- æ¨¡å‹æ€§èƒ½åˆ†æ
"""

import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Tuple, Any, Optional
import warnings
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
import joblib
import optuna
from datetime import datetime
import os
from tqdm import tqdm
import time

warnings.filterwarnings('ignore')

class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict):
        """
        åˆå§‹åŒ–æ¨¡å‹è®­ç»ƒå™¨
        
        å‚æ•°:
            config (Dict): é…ç½®ä¿¡æ¯
        """
        self.config = config
        self.ml_config = config.get("ml_signal_enhancer", {})
        self.trainer_config = self.ml_config.get("model_trainer", {})
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # è®­ç»ƒé…ç½®
        self.test_size = self.trainer_config.get("test_size", 0.2)
        self.validation_size = self.trainer_config.get("validation_size", 0.2)
        self.random_state = self.trainer_config.get("random_state", 42)
        self.cv_folds = self.trainer_config.get("cv_folds", 5)
        self.enable_hyperopt = self.trainer_config.get("enable_hyperopt", True)
        self.n_trials = self.trainer_config.get("optuna_trials", 100)
        
        # æ¨¡å‹ä¿å­˜è·¯å¾„
        self.model_save_path = self.trainer_config.get("model_save_path", "models/")
        os.makedirs(self.model_save_path, exist_ok=True)
        
        # æ”¯æŒçš„æ¨¡å‹ç±»å‹
        self.model_types = self.trainer_config.get("model_types", [
            "random_forest", "xgboost", "lightgbm", "logistic_regression"
        ])
        
        # å·²è®­ç»ƒçš„æ¨¡å‹
        self.trained_models = {}
        self.model_metrics = {}
        
        self.logger.info("æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def train_models(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """
        è®­ç»ƒå¤šä¸ªæœºå™¨å­¦ä¹ æ¨¡å‹
        
        å‚æ•°:
            X (pd.DataFrame): ç‰¹å¾æ•°æ®
            y (pd.Series): ç›®æ ‡å˜é‡
            
        è¿”å›:
            Dict: è®­ç»ƒç»“æœ
        """
        self.logger.info("å¼€å§‹è®­ç»ƒå¤šä¸ªMLæ¨¡å‹...")
        start_time = time.time()
        
        try:
            # æ•°æ®åˆ’åˆ†
            print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
            X_train, X_test, y_train, y_test = self._split_data(X, y)
            print(f"   â€¢ è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
            print(f"   â€¢ æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
            
            models = {}
            metrics = {}
            
            # ä½¿ç”¨è¿›åº¦æ¡æ˜¾ç¤ºæ¨¡å‹è®­ç»ƒè¿›ç¨‹
            print(f"\nğŸ¤– å¼€å§‹è®­ç»ƒ {len(self.model_types)} ä¸ªæ¨¡å‹")
            with tqdm(total=len(self.model_types), desc="ğŸ”¥ æ¨¡å‹è®­ç»ƒè¿›åº¦", 
                     bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]",
                     ncols=100) as pbar:
                
                for model_type in self.model_types:
                    model_start = time.time()
                    pbar.set_description(f"ğŸ”¥ è®­ç»ƒ{model_type}")
                    
                    self.logger.info(f"è®­ç»ƒ {model_type} æ¨¡å‹...")
                    
                    # è·å–æ¨¡å‹å’Œå‚æ•°
                    model, param_grid = self._get_model_and_params(model_type)
                    
                    # è¶…å‚æ•°ä¼˜åŒ–
                    if self.enable_hyperopt:
                        best_model = self._optimize_hyperparameters(
                            model, X_train, y_train, model_type
                        )
                    else:
                        best_model = model.fit(X_train, y_train)
                    
                    # è¯„ä¼°æ¨¡å‹
                    model_metrics = self._evaluate_model(
                        best_model, X_test, y_test, model_type
                    )
                    
                    models[model_type] = best_model
                    metrics[model_type] = model_metrics
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    model_time = time.time() - model_start
                    accuracy = model_metrics['accuracy']
                    pbar.set_postfix({"å‡†ç¡®ç‡": f"{accuracy:.1%}", "è€—æ—¶": f"{model_time:.1f}s"})
                    pbar.update(1)
                    
                    self.logger.info(f"{model_type} è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {accuracy:.4f}")
            
            print(f"\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
            # ä¿å­˜æ¨¡å‹
            self._save_trained_models(models)
            
            # æ›´æ–°å†…éƒ¨çŠ¶æ€
            self.trained_models = models
            self.model_metrics = metrics
            
            # è®­ç»ƒæ€»ç»“
            total_time = time.time() - start_time
            best_model_name = self._select_best_model(metrics)
            best_accuracy = metrics[best_model_name]['accuracy']
            
            print(f"\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")
            print(f"   â€¢ æ€»è€—æ—¶: {total_time:.1f}ç§’")
            print(f"   â€¢ æœ€ä½³æ¨¡å‹: {best_model_name} (å‡†ç¡®ç‡: {best_accuracy:.1%})")
            print(f"   â€¢ è®­ç»ƒæ ·æœ¬: {len(X_train)} ä¸ª")
            print(f"   â€¢ æµ‹è¯•æ ·æœ¬: {len(X_test)} ä¸ª")
            
            results = {
                'success': True,
                'models': models,
                'metrics': metrics,
                'best_model': best_model_name,
                'training_samples': len(X_train),
                'test_samples': len(X_test),
                'total_time': total_time,
                'feature_count': X.shape[1]
            }
            
            self.logger.info("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ")
            return results
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def update_models(self, X_new: pd.DataFrame, y_new: pd.Series) -> Dict:
        """
        å¢é‡æ›´æ–°å·²è®­ç»ƒçš„æ¨¡å‹
        
        å‚æ•°:
            X_new (pd.DataFrame): æ–°çš„ç‰¹å¾æ•°æ®
            y_new (pd.Series): æ–°çš„ç›®æ ‡å˜é‡
            
        è¿”å›:
            Dict: æ›´æ–°ç»“æœ
        """
        if not self.trained_models:
            self.logger.warning("æ²¡æœ‰å·²è®­ç»ƒçš„æ¨¡å‹å¯ä»¥æ›´æ–°")
            return {'success': False, 'error': 'æ²¡æœ‰å·²è®­ç»ƒçš„æ¨¡å‹'}
        
        try:
            self.logger.info("å¼€å§‹å¢é‡æ›´æ–°æ¨¡å‹...")
            
            updated_models = {}
            updated_metrics = {}
            
            # å‡†å¤‡éªŒè¯æ•°æ®
            X_train, X_val, y_train, y_val = train_test_split(
                X_new, y_new, test_size=0.3, random_state=self.random_state
            )
            
            for model_type, model in self.trained_models.items():
                self.logger.info(f"æ›´æ–° {model_type} æ¨¡å‹...")
                
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ›´æ–°ç­–ç•¥
                if hasattr(model, 'partial_fit'):
                    # æ”¯æŒå¢é‡å­¦ä¹ çš„æ¨¡å‹
                    updated_model = model.partial_fit(X_train, y_train)
                elif model_type in ['xgboost', 'lightgbm']:
                    # åŸºäºæ ‘çš„æ¨¡å‹é‡æ–°è®­ç»ƒ
                    updated_model = self._retrain_tree_model(
                        model, X_train, y_train, model_type
                    )
                else:
                    # å…¶ä»–æ¨¡å‹é‡æ–°è®­ç»ƒ
                    updated_model = model.fit(X_train, y_train)
                
                # è¯„ä¼°æ›´æ–°åçš„æ¨¡å‹
                new_metrics = self._evaluate_model(
                    updated_model, X_val, y_val, model_type
                )
                
                updated_models[model_type] = updated_model
                updated_metrics[model_type] = new_metrics
                
                self.logger.info(f"{model_type} æ›´æ–°å®Œæˆï¼Œæ–°å‡†ç¡®ç‡: {new_metrics['accuracy']:.4f}")
            
            results = {
                'success': True,
                'updated_models': updated_models,
                'metrics': updated_metrics,
                'improvement_summary': self._compare_performance(updated_metrics)
            }
            
            return results
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹æ›´æ–°å¤±è´¥: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def _split_data(self, X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """åˆ’åˆ†è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
        # ä½¿ç”¨æ—¶é—´åºåˆ—åˆ’åˆ†é¿å…æ•°æ®æ³„æ¼
        split_idx = int(len(X) * (1 - self.test_size))
        
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]
        
        return X_train, X_test, y_train, y_test
    
    def _get_model_and_params(self, model_type: str) -> Tuple[Any, Dict]:
        """è·å–æ¨¡å‹å®ä¾‹å’Œè¶…å‚æ•°ç½‘æ ¼"""
        if model_type == "random_forest":
            model = RandomForestClassifier(random_state=self.random_state)
            params = {
                'n_estimators': [100, 200, 300],
                'max_depth': [10, 20, None],
                'min_samples_split': [2, 5, 10],
                'min_samples_leaf': [1, 2, 4]
            }
        elif model_type == "xgboost":
            model = xgb.XGBClassifier(random_state=self.random_state)
            params = {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 6, 10],
                'learning_rate': [0.01, 0.1, 0.2],
                'subsample': [0.8, 0.9, 1.0]
            }
        elif model_type == "lightgbm":
            model = lgb.LGBMClassifier(random_state=self.random_state, verbose=-1)
            params = {
                'n_estimators': [100, 200, 300],
                'max_depth': [3, 6, 10],
                'learning_rate': [0.01, 0.1, 0.2],
                'num_leaves': [31, 50, 100]
            }
        elif model_type == "logistic_regression":
            model = LogisticRegression(random_state=self.random_state, max_iter=1000)
            params = {
                'C': [0.001, 0.01, 0.1, 1, 10],
                'penalty': ['l1', 'l2'],
                'solver': ['liblinear', 'saga']
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
        
        return model, params
    
    def _optimize_hyperparameters(self, model: Any, X_train: pd.DataFrame, 
                                y_train: pd.Series, model_type: str) -> Any:
        """ä½¿ç”¨Optunaè¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–"""
        def objective(trial):
            if model_type == "random_forest":
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 5, 20),
                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
                }
                clf = RandomForestClassifier(random_state=self.random_state, **params)
            elif model_type == "xgboost":
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0)
                }
                clf = xgb.XGBClassifier(random_state=self.random_state, **params)
            elif model_type == "lightgbm":
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 50, 300),
                    'max_depth': trial.suggest_int('max_depth', 3, 10),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
                    'num_leaves': trial.suggest_int('num_leaves', 20, 100)
                }
                clf = lgb.LGBMClassifier(random_state=self.random_state, verbose=-1, **params)
            elif model_type == "logistic_regression":
                params = {
                    'C': trial.suggest_float('C', 0.001, 10, log=True),
                    'penalty': trial.suggest_categorical('penalty', ['l1', 'l2'])
                }
                if params['penalty'] == 'l1':
                    params['solver'] = 'liblinear'
                else:
                    params['solver'] = 'lbfgs'
                clf = LogisticRegression(random_state=self.random_state, max_iter=1000, **params)
            
            # æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
            tscv = TimeSeriesSplit(n_splits=3)
            scores = cross_val_score(clf, X_train, y_train, cv=tscv, scoring='accuracy')
            return scores.mean()
        
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=self.n_trials, show_progress_bar=False)
        
        # ä½¿ç”¨æœ€ä¼˜å‚æ•°è®­ç»ƒæ¨¡å‹
        best_params = study.best_params
        if model_type == "random_forest":
            best_model = RandomForestClassifier(random_state=self.random_state, **best_params)
        elif model_type == "xgboost":
            best_model = xgb.XGBClassifier(random_state=self.random_state, **best_params)
        elif model_type == "lightgbm":
            best_model = lgb.LGBMClassifier(random_state=self.random_state, verbose=-1, **best_params)
        elif model_type == "logistic_regression":
            best_model = LogisticRegression(random_state=self.random_state, max_iter=1000, **best_params)
        
        return best_model.fit(X_train, y_train)
    
    def _evaluate_model(self, model: Any, X_test: pd.DataFrame, 
                       y_test: pd.Series, model_type: str) -> Dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, average='weighted'),
            'recall': recall_score(y_test, y_pred, average='weighted'),
            'f1_score': f1_score(y_test, y_pred, average='weighted')
        }
        
        if y_pred_proba is not None:
            metrics['auc_roc'] = roc_auc_score(y_test, y_pred_proba)
        
        return metrics
    
    def _select_best_model(self, metrics: Dict) -> str:
        """é€‰æ‹©æœ€ä½³æ¨¡å‹"""
        best_model = None
        best_score = 0
        
        for model_type, model_metrics in metrics.items():
            # ç»¼åˆè¯„åˆ†ï¼ˆå¯ä»¥è°ƒæ•´æƒé‡ï¼‰
            score = (model_metrics['accuracy'] * 0.4 + 
                    model_metrics['f1_score'] * 0.3 + 
                    model_metrics.get('auc_roc', 0) * 0.3)
            
            if score > best_score:
                best_score = score
                best_model = model_type
        
        return best_model
    
    def _save_trained_models(self, models: Dict):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        for model_type, model in models.items():
            filename = f"{model_type}_{timestamp}.joblib"
            filepath = os.path.join(self.model_save_path, filename)
            joblib.dump(model, filepath)
            self.logger.info(f"æ¨¡å‹å·²ä¿å­˜: {filepath}")
    
    def _retrain_tree_model(self, model: Any, X_new: pd.DataFrame, 
                          y_new: pd.Series, model_type: str) -> Any:
        """é‡æ–°è®­ç»ƒåŸºäºæ ‘çš„æ¨¡å‹"""
        # è·å–åŸå§‹å‚æ•°
        params = model.get_params()
        
        # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
        if model_type == "xgboost":
            new_model = xgb.XGBClassifier(**params)
        elif model_type == "lightgbm":
            new_model = lgb.LGBMClassifier(**params)
        else:
            new_model = model.__class__(**params)
        
        return new_model.fit(X_new, y_new)
    
    def _compare_performance(self, new_metrics: Dict) -> Dict:
        """æ¯”è¾ƒæ–°æ—§æ¨¡å‹æ€§èƒ½"""
        improvements = {}
        
        for model_type, new_metric in new_metrics.items():
            if model_type in self.model_metrics:
                old_metric = self.model_metrics[model_type]
                
                accuracy_change = new_metric['accuracy'] - old_metric['accuracy']
                f1_change = new_metric['f1_score'] - old_metric['f1_score']
                
                improvements[model_type] = {
                    'accuracy_change': accuracy_change,
                    'f1_change': f1_change,
                    'improved': accuracy_change > 0 and f1_change > 0
                }
        
        return improvements
    
    def get_model_info(self) -> Dict:
        """è·å–å·²è®­ç»ƒæ¨¡å‹ä¿¡æ¯"""
        return {
            'trained_models': list(self.trained_models.keys()),
            'model_metrics': self.model_metrics,
            'best_model': self._select_best_model(self.model_metrics) if self.model_metrics else None
        }